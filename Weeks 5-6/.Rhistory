# Ridge regression model
model_ridge <- glmnet(X_train_matrix, y_train_vector, alpha = 0, lambda = best_lambda_ridge)
# Lasso regression model
model_lasso <- glmnet(X_train_matrix, y_train_vector, alpha = 1, lambda = best_lambda_lasso)
# ElasticNet regression model
model_enet <- glmnet(X_train_matrix, y_train_vector, alpha = 0.5, lambda = best_lambda_enet)
# Predictions
y_pred_lm <- predict(model_lm, newdata = as.data.frame(X_test))
y_pred_ridge <- predict(model_ridge, s = best_lambda_ridge, newx = X_test_matrix)
y_pred_lasso <- predict(model_lasso, s = best_lambda_lasso, newx = X_test_matrix)
y_pred_enet <- predict(model_enet, s = best_lambda_enet, newx = X_test_matrix)
# Evaluation metrics
mse_lm <- mean((y_test - y_pred_lm)^2)
r2_lm <- summary(model_lm)$r.squared
mse_ridge <- mean((y_test - y_pred_ridge)^2)
r2_ridge <- 1 - (sum((y_test - y_pred_ridge)^2) / sum((y_test - mean(y_test))^2))
mse_lasso <- mean((y_test - y_pred_lasso)^2)
r2_lasso <- 1 - (sum((y_test - y_pred_lasso)^2) / sum((y_test - mean(y_test))^2))
mse_enet <- mean((y_test - y_pred_enet)^2)
r2_enet <- 1 - (sum((y_test - y_pred_enet)^2) / sum((y_test - mean(y_test))^2))
# Output the results for each model
cat("\nLinear Regression:\n")
print(summary(model_lm)$coefficients)
cat(paste("Mean squared error:", mse_lm, "\n"))
cat(paste("R-squared value:", r2_lm, "\n"))
cat("\nRidge Regression:\n")
print(coef(model_ridge))
cat(paste("Mean squared error:", mse_ridge, "\n"))
cat(paste("R-squared value:", r2_ridge, "\n"))
cat("\nLasso Regression:\n")
print(coef(model_lasso))
cat(paste("Mean squared error:", mse_lasso, "\n"))
cat(paste("R-squared value:", r2_lasso, "\n"))
cat("\nElasticNet Regression:\n")
print(coef(model_enet))
cat(paste("Mean squared error:", mse_enet, "\n"))
cat(paste("R-squared value:", r2_enet, "\n"))
# Load necessary libraries
library(dplyr)
library(caret)
library(glmnet)
# Create the dataset
data <- data.frame(
Region = c('North', 'South', 'East', 'West', 'North', 'South', 'East', 'West', 'North', 'South', 'East', 'West', 'North', 'South', 'East', 'West'),
Dealership_Size = c(2500, 3000, 1800, 2200, 2700, 3200, 1900, 2300, 2600, 3100, 2000, 2400, 2800, 3300, 2100, 2500),
Marketing_Spend = c(50000, 60000, 45000, 48000, 52000, 62000, 47000, 49000, 51000, 61000, 46000, 50000, 53000, 63000, 48000, 52000),
Customer_Interactions = c(300, 400, 280, 310, 330, 420, 290, 320, 350, 430, 300, 340, 370, 440, 310, 360),
Sales = c(120, 150, 100, 110, 130, 160, 105, 115, 125, 155, 110, 120, 140, 170, 115, 130)
)
# Data Cleaning Tasks
data <- data %>%
mutate(Customer_Interactions = ifelse(is.na(Customer_Interactions), mean(Customer_Interactions, na.rm = TRUE), Customer_Interactions)) %>%
distinct()
data$Marketing_Spend <- as.numeric(data$Marketing_Spend)
threshold <- quantile(data$Sales, 0.95)
data <- filter(data, Sales < threshold)
data <- data %>%
mutate(across(Region, as.factor)) %>%
mutate(Region = relevel(Region, ref = "East")) %>%
model.matrix(~ Region - 1, data = .) %>%
cbind(data, .) %>%
select(-Region)
X <- data %>%
select(Dealership_Size, Marketing_Spend, Customer_Interactions, RegionNorth, RegionWest, RegionSouth)
y <- data$Sales
# Split the data into training and testing sets
set.seed(42)
trainIndex <- createDataPartition(y, p = 0.8, list = FALSE)
X_train <- X[trainIndex,]
X_test <- X[-trainIndex,]
y_train <- y[trainIndex]
y_test <- y[-trainIndex]
# Standardize the data
scaler <- preProcess(X_train, method = c("center", "scale"))
X_train_scaled <- predict(scaler, X_train)
X_test_scaled <- predict(scaler, X_test)
X_train_matrix <- as.matrix(X_train_scaled)
X_test_matrix <- as.matrix(X_test_scaled)
y_train_vector <- as.vector(y_train)
y_test_vector <- as.vector(y_test)
# Linear regression model
model_lm <- lm(y_train ~ ., data = as.data.frame(cbind(X_train_scaled, y_train)))
# Cross-validation for Ridge, Lasso, and ElasticNet to find optimal lambda
cv_ridge <- cv.glmnet(X_train_matrix, y_train_vector, alpha = 0)
cv_lasso <- cv.glmnet(X_train_matrix, y_train_vector, alpha = 1)
cv_enet <- cv.glmnet(X_train_matrix, y_train_vector, alpha = 0.5)
# Get best lambda
best_lambda_ridge <- cv_ridge$lambda.min
best_lambda_lasso <- cv_lasso$lambda.min
best_lambda_enet <- cv_enet$lambda.min
# Ridge regression model
model_ridge <- glmnet(X_train_matrix, y_train_vector, alpha = 0, lambda = best_lambda_ridge)
# Lasso regression model
model_lasso <- glmnet(X_train_matrix, y_train_vector, alpha = 1, lambda = best_lambda_lasso)
# ElasticNet regression model
model_enet <- glmnet(X_train_matrix, y_train_vector, alpha = 0.5, lambda = best_lambda_enet)
# Predictions
y_pred_lm <- predict(model_lm, newdata = as.data.frame(X_test_scaled))
y_pred_ridge <- predict(model_ridge, s = best_lambda_ridge, newx = X_test_matrix)
y_pred_lasso <- predict(model_lasso, s = best_lambda_lasso, newx = X_test_matrix)
y_pred_enet <- predict(model_enet, s = best_lambda_enet, newx = X_test_matrix)
# Evaluation metrics
mse_lm <- mean((y_test - y_pred_lm)^2)
r2_lm <- summary(model_lm)$r.squared
mse_ridge <- mean((y_test - y_pred_ridge)^2)
r2_ridge <- 1 - (sum((y_test - y_pred_ridge)^2) / sum((y_test - mean(y_test))^2))
mse_lasso <- mean((y_test - y_pred_lasso)^2)
r2_lasso <- 1 - (sum((y_test - y_pred_lasso)^2) / sum((y_test - mean(y_test))^2))
mse_enet <- mean((y_test - y_pred_enet)^2)
r2_enet <- 1 - (sum((y_test - y_pred_enet)^2) / sum((y_test - mean(y_test))^2))
# Output the results for each model
cat("\nLinear Regression:\n")
print(summary(model_lm)$coefficients)
cat(paste("Mean squared error:", mse_lm, "\n"))
cat(paste("R-squared value:", r2_lm, "\n"))
cat("\nRidge Regression:\n")
print(coef(model_ridge))
cat(paste("Mean squared error:", mse_ridge, "\n"))
cat(paste("R-squared value:", r2_ridge, "\n"))
cat("\nLasso Regression:\n")
print(coef(model_lasso))
cat(paste("Mean squared error:", mse_lasso, "\n"))
cat(paste("R-squared value:", r2_lasso, "\n"))
cat("\nElasticNet Regression:\n")
print(coef(model_enet))
cat(paste("Mean squared error:", mse_enet, "\n"))
cat(paste("R-squared value:", r2_enet, "\n"))
# Load necessary libraries
library(dplyr)
library(caret)
library(glmnet)
# Create the dataset
data <- data.frame(
Region = c('North', 'South', 'East', 'West', 'North', 'South', 'East', 'West', 'North', 'South', 'East', 'West', 'North', 'South', 'East', 'West'),
Dealership_Size = c(2500, 3000, 1800, 2200, 2700, 3200, 1900, 2300, 2600, 3100, 2000, 2400, 2800, 3300, 2100, 2500),
Marketing_Spend = c(50000, 60000, 45000, 48000, 52000, 62000, 47000, 49000, 51000, 61000, 46000, 50000, 53000, 63000, 48000, 52000),
Customer_Interactions = c(300, 400, 280, 310, 330, 420, 290, 320, 350, 430, 300, 340, 370, 440, 310, 360),
Sales = c(120, 150, 100, 110, 130, 160, 105, 115, 125, 155, 110, 120, 140, 170, 115, 130)
)
# Data Cleaning Tasks
data <- data %>%
mutate(Customer_Interactions = ifelse(is.na(Customer_Interactions), mean(Customer_Interactions, na.rm = TRUE), Customer_Interactions)) %>%
distinct()
data$Marketing_Spend <- as.numeric(data$Marketing_Spend)
threshold <- quantile(data$Sales, 0.95)
data <- filter(data, Sales < threshold)
data <- data %>%
mutate(across(Region, as.factor)) %>%
mutate(Region = relevel(Region, ref = "East")) %>%
model.matrix(~ Region - 1, data = .) %>%
cbind(data, .) %>%
select(-Region)
X <- data %>%
select(Dealership_Size, Marketing_Spend, Customer_Interactions, RegionNorth, RegionWest, RegionSouth)
y <- data$Sales
# Split the data into training and testing sets
set.seed(42)
trainIndex <- createDataPartition(y, p = 0.8, list = FALSE)
X_train <- X[trainIndex,]
X_test <- X[-trainIndex,]
y_train <- y[trainIndex]
y_test <- y[-trainIndex]
# Standardize the data
scaler <- preProcess(X_train, method = c("center", "scale"))
X_train_scaled <- predict(scaler, X_train)
X_test_scaled <- predict(scaler, X_test)
X_train_matrix <- as.matrix(X_train_scaled)
X_test_matrix <- as.matrix(X_test_scaled)
y_train_vector <- as.vector(y_train)
y_test_vector <- as.vector(y_test)
# Linear regression model
model_lm <- lm(y_train ~ ., data = as.data.frame(cbind(X_train_scaled, y_train)))
# Cross-validation for Ridge, Lasso, and ElasticNet to find optimal lambda
cv_ridge <- cv.glmnet(X_train_matrix, y_train_vector, alpha = 0)
cv_lasso <- cv.glmnet(X_train_matrix, y_train_vector, alpha = 1)
cv_enet <- cv.glmnet(X_train_matrix, y_train_vector, alpha = 0.5)
# Get best lambda
best_lambda_ridge <- cv_ridge$lambda.min
best_lambda_lasso <- cv_lasso$lambda.min
best_lambda_enet <- cv_enet$lambda.min
# Check the best lambda values
cat("Best lambda for Ridge:", best_lambda_ridge, "\n")
cat("Best lambda for Lasso:", best_lambda_lasso, "\n")
cat("Best lambda for ElasticNet:", best_lambda_enet, "\n")
# Ridge regression model
model_ridge <- glmnet(X_train_matrix, y_train_vector, alpha = 0, lambda = best_lambda_ridge)
# Lasso regression model
model_lasso <- glmnet(X_train_matrix, y_train_vector, alpha = 1, lambda = best_lambda_lasso)
# ElasticNet regression model
model_enet <- glmnet(X_train_matrix, y_train_vector, alpha = 0.5, lambda = best_lambda_enet)
# Predictions
y_pred_lm <- predict(model_lm, newdata = as.data.frame(X_test_scaled))
y_pred_ridge <- predict(model_ridge, s = best_lambda_ridge, newx = X_test_matrix)
y_pred_lasso <- predict(model_lasso, s = best_lambda_lasso, newx = X_test_matrix)
y_pred_enet <- predict(model_enet, s = best_lambda_enet, newx = X_test_matrix)
# Check for NaN values in predictions
cat("NaN values in Lasso predictions:", sum(is.na(y_pred_lasso)), "\n")
cat("NaN values in ElasticNet predictions:", sum(is.na(y_pred_enet)), "\n")
# Evaluation metrics
mse_lm <- mean((y_test - y_pred_lm)^2)
r2_lm <- summary(model_lm)$r.squared
if (all(!is.na(y_pred_ridge))) {
mse_ridge <- mean((y_test - y_pred_ridge)^2)
r2_ridge <- 1 - (sum((y_test - y_pred_ridge)^2) / sum((y_test - mean(y_test))^2))
} else {
mse_ridge <- NA
r2_ridge <- NA
}
if (all(!is.na(y_pred_lasso))) {
mse_lasso <- mean((y_test - y_pred_lasso)^2)
r2_lasso <- 1 - (sum((y_test - y_pred_lasso)^2) / sum((y_test - mean(y_test))^2))
} else {
mse_lasso <- NA
r2_lasso <- NA
}
if (all(!is.na(y_pred_enet))) {
mse_enet <- mean((y_test - y_pred_enet)^2)
r2_enet <- 1 - (sum((y_test - y_pred_enet)^2) / sum((y_test - mean(y_test))^2))
} else {
mse_enet <- NA
r2_enet <- NA
}
# Output the results for each model
cat("\nLinear Regression:\n")
print(summary(model_lm)$coefficients)
cat(paste("Mean squared error:", mse_lm, "\n"))
cat(paste("R-squared value:", r2_lm, "\n"))
cat("\nRidge Regression:\n")
print(coef(model_ridge))
cat(paste("Mean squared error:", mse_ridge, "\n"))
cat(paste("R-squared value:", r2_ridge, "\n"))
cat("\nLasso Regression:\n")
print(coef(model_lasso))
cat(paste("Mean squared error:", mse_lasso, "\n"))
cat(paste("R-squared value:", r2_lasso, "\n"))
cat("\nElasticNet Regression:\n")
print(coef(model_enet))
cat(paste("Mean squared error:", mse_enet, "\n"))
cat(paste("R-squared value:", r2_enet, "\n"))
# Load necessary libraries
library(dplyr)
library(glmnet)
library(caret)
# Create the dataset
data <- data.frame(
Region = c('North', 'South', 'East', 'West', 'North', 'South', 'East', 'West', 'North', 'South', 'East', 'West', 'North', 'South', 'East', 'West'),
Dealership_Size = c(2500, 3000, 1800, 2200, 2700, 3200, 1900, 2300, 2600, 3100, 2000, 2400, 2800, 3300, 2100, 2500),
Marketing_Spend = c(50000, 60000, 45000, 48000, 52000, 62000, 47000, 49000, 51000, 61000, 46000, 50000, 53000, 63000, 48000, 52000),
Customer_Interactions = c(300, 400, 280, 310, 330, 420, 290, 320, 350, 430, 300, 340, 370, 440, 310, 360),
Sales = c(120, 150, 100, 110, 130, 160, 105, 115, 125, 155, 110, 120, 140, 170, 115, 130)
)
print(data)
# Data Cleaning Tasks
# Check for missing values and handle them
print("Missing values in each column:")
print(colSums(is.na(data)))
# Remove duplicate rows if any
data <- data %>%
distinct()
# Ensure consistent data formats
data$Marketing_Spend <- as.numeric(data$Marketing_Spend)
# Convert categorical variable to dummy variables
data <- model.matrix(~ Region - 1, data)
data <- cbind(data, data)
data <- as.data.frame(data)
data <- select(data, -Region)
# Load necessary libraries
library(dplyr)
library(glmnet)
library(caret)
# Create the dataset
data <- data.frame(
Region = c('North', 'South', 'East', 'West', 'North', 'South', 'East', 'West', 'North', 'South', 'East', 'West', 'North', 'South', 'East', 'West'),
Dealership_Size = c(2500, 3000, 1800, 2200, 2700, 3200, 1900, 2300, 2600, 3100, 2000, 2400, 2800, 3300, 2100, 2500),
Marketing_Spend = c(50000, 60000, 45000, 48000, 52000, 62000, 47000, 49000, 51000, 61000, 46000, 50000, 53000, 63000, 48000, 52000),
Customer_Interactions = c(300, 400, 280, 310, 330, 420, 290, 320, 350, 430, 300, 340, 370, 440, 310, 360),
Sales = c(120, 150, 100, 110, 130, 160, 105, 115, 125, 155, 110, 120, 140, 170, 115, 130)
)
print(data)
# Data Cleaning Tasks
# Check for missing values and handle them
print("Missing values in each column:")
print(colSums(is.na(data)))
# Remove duplicate rows if any
data <- data %>%
distinct()
# Ensure consistent data formats
data$Marketing_Spend <- as.numeric(data$Marketing_Spend)
# Convert categorical variable to dummy variables
region_dummies <- model.matrix(~ Region - 1, data)
data <- cbind(data, region_dummies)
# Drop the original 'Region' column
data <- data %>%
select(-Region)
# Define the dependent variable (DV) and independent variables (IVs)
X <- data %>%
select(Dealership_Size, Marketing_Spend, Customer_Interactions, RegionNorth, RegionWest, RegionSouth)
y <- data$Sales
# Split the data into training and testing sets
set.seed(42)
trainIndex <- createDataPartition(y, p = 0.8, list = FALSE)
X_train <- X[trainIndex, ]
X_test <- X[-trainIndex, ]
y_train <- y[trainIndex]
y_test <- y[-trainIndex]
# Standardize the data
scaler <- preProcess(X_train, method = c("center", "scale"))
X_train_scaled <- predict(scaler, X_train)
X_test_scaled <- predict(scaler, X_test)
# Convert to matrices for glmnet
X_train_matrix <- as.matrix(X_train_scaled)
X_test_matrix <- as.matrix(X_test_scaled)
y_train_vector <- as.vector(y_train)
# Create and fit models
results <- list()
# Linear Regression
model_lm <- lm(y_train ~ ., data = as.data.frame(cbind(X_train_scaled, y_train)))
y_pred_lm <- predict(model_lm, newdata = as.data.frame(X_test_scaled))
mse_lm <- mean((y_test - y_pred_lm)^2)
r2_lm <- summary(model_lm)$r.squared
results[["Linear Regression"]] <- list(Coefficients = coef(model_lm), Intercept = coef(model_lm)[1], `Mean squared error` = mse_lm, `R-squared value` = r2_lm)
# Ridge Regression
model_ridge <- cv.glmnet(X_train_matrix, y_train_vector, alpha = 0)
y_pred_ridge <- predict(model_ridge, s = model_ridge$lambda.min, newx = X_test_matrix)
mse_ridge <- mean((y_test - y_pred_ridge)^2)
r2_ridge <- 1 - (sum((y_test - y_pred_ridge)^2) / sum((y_test - mean(y_test))^2))
results[["Ridge Regression"]] <- list(Coefficients = coef(model_ridge, s = model_ridge$lambda.min), `Mean squared error` = mse_ridge, `R-squared value` = r2_ridge)
# Lasso Regression
model_lasso <- cv.glmnet(X_train_matrix, y_train_vector, alpha = 1)
y_pred_lasso <- predict(model_lasso, s = model_lasso$lambda.min, newx = X_test_matrix)
mse_lasso <- mean((y_test - y_pred_lasso)^2)
r2_lasso <- 1 - (sum((y_test - y_pred_lasso)^2) / sum((y_test - mean(y_test))^2))
results[["Lasso Regression"]] <- list(Coefficients = coef(model_lasso, s = model_lasso$lambda.min), `Mean squared error` = mse_lasso, `R-squared value` = r2_lasso)
# ElasticNet Regression
model_enet <- cv.glmnet(X_train_matrix, y_train_vector, alpha = 0.5)
y_pred_enet <- predict(model_enet, s = model_enet$lambda.min, newx = X_test_matrix)
mse_enet <- mean((y_test - y_pred_enet)^2)
r2_enet <- 1 - (sum((y_test - y_pred_enet)^2) / sum((y_test - mean(y_test))^2))
results[["ElasticNet Regression"]] <- list(Coefficients = coef(model_enet, s = model_enet$lambda.min), `Mean squared error` = mse_enet, `R-squared value` = r2_enet)
# Output the results for each model
for (name in names(results)) {
cat("\n", name, ":\n", sep = "")
print(results[[name]])
}
# Load necessary libraries
library(dplyr)
library(glmnet)
library(caret)
# Create the dataset
data <- data.frame(
Region = c('North', 'South', 'East', 'West', 'North', 'South', 'East', 'West', 'North', 'South', 'East', 'West', 'North', 'South', 'East', 'West'),
Dealership_Size = c(2500, 3000, 1800, 2200, 2700, 3200, 1900, 2300, 2600, 3100, 2000, 2400, 2800, 3300, 2100, 2500),
Marketing_Spend = c(50000, 60000, 45000, 48000, 52000, 62000, 47000, 49000, 51000, 61000, 46000, 50000, 53000, 63000, 48000, 52000),
Customer_Interactions = c(300, 400, 280, 310, 330, 420, 290, 320, 350, 430, 300, 340, 370, 440, 310, 360),
Sales = c(120, 150, 100, 110, 130, 160, 105, 115, 125, 155, 110, 120, 140, 170, 115, 130)
)
# Data Cleaning Tasks
# Check for missing values and handle them
print("Missing values in each column:")
print(colSums(is.na(data)))
# Remove duplicate rows if any
data <- data %>%
distinct()
# Ensure consistent data formats
data$Marketing_Spend <- as.numeric(data$Marketing_Spend)
# Convert categorical variable to dummy variables
region_dummies <- model.matrix(~ Region - 1, data)
data <- cbind(data, region_dummies)
# Drop the original 'Region' column
data <- data %>%
select(-Region)
# Define the dependent variable (DV) and independent variables (IVs)
X <- data %>%
select(Dealership_Size, Marketing_Spend, Customer_Interactions, RegionNorth, RegionWest, RegionSouth)
y <- data$Sales
# Split the data into training and testing sets
set.seed(42)
trainIndex <- createDataPartition(y, p = 0.8, list = FALSE)
X_train <- X[trainIndex, ]
X_test <- X[-trainIndex, ]
y_train <- y[trainIndex]
y_test <- y[-trainIndex]
# Standardize the data
scaler <- preProcess(X_train, method = c("center", "scale"))
X_train_scaled <- predict(scaler, X_train)
X_test_scaled <- predict(scaler, X_test)
# Convert to matrices for glmnet
X_train_matrix <- as.matrix(X_train_scaled)
X_test_matrix <- as.matrix(X_test_scaled)
y_train_vector <- as.vector(y_train)
# Create and fit models
results <- list()
# Linear Regression
model_lm <- lm(y_train ~ ., data = as.data.frame(cbind(X_train_scaled, y_train)))
y_pred_lm <- predict(model_lm, newdata = as.data.frame(X_test_scaled))
mse_lm <- mean((y_test - y_pred_lm)^2)
r2_lm <- summary(model_lm)$r.squared
results[["Linear Regression"]] <- list(Coefficients = coef(model_lm), Intercept = coef(model_lm)[1], `Mean squared error` = mse_lm, `R-squared value` = r2_lm)
# Ridge Regression
model_ridge <- cv.glmnet(X_train_matrix, y_train_vector, alpha = 0)
y_pred_ridge <- predict(model_ridge, s = model_ridge$lambda.min, newx = X_test_matrix)
mse_ridge <- mean((y_test - y_pred_ridge)^2)
r2_ridge <- 1 - (sum((y_test - y_pred_ridge)^2) / sum((y_test - mean(y_test))^2))
results[["Ridge Regression"]] <- list(Coefficients = coef(model_ridge, s = model_ridge$lambda.min), `Mean squared error` = mse_ridge, `R-squared value` = r2_ridge)
# Lasso Regression
model_lasso <- cv.glmnet(X_train_matrix, y_train_vector, alpha = 1)
y_pred_lasso <- predict(model_lasso, s = model_lasso$lambda.min, newx = X_test_matrix)
cat("Lasso Predictions:", y_pred_lasso, "\n")
mse_lasso <- mean((y_test - y_pred_lasso)^2)
r2_lasso <- 1 - (sum((y_test - y_pred_lasso)^2) / sum((y_test - mean(y_test))^2))
results[["Lasso Regression"]] <- list(Coefficients = coef(model_lasso, s = model_lasso$lambda.min), `Mean squared error` = mse_lasso, `R-squared value` = r2_lasso)
# ElasticNet Regression
model_enet <- cv.glmnet(X_train_matrix, y_train_vector, alpha = 0.5)
y_pred_enet <- predict(model_enet, s = model_enet$lambda.min, newx = X_test_matrix)
cat("ElasticNet Predictions:", y_pred_enet, "\n")
mse_enet <- mean((y_test - y_pred_enet)^2)
r2_enet <- 1 - (sum((y_test - y_pred_enet)^2) / sum((y_test - mean(y_test))^2))
results[["ElasticNet Regression"]] <- list(Coefficients = coef(model_enet, s = model_enet$lambda.min), `Mean squared error` = mse_enet, `R-squared value` = r2_enet)
# Output the results for each model
for (name in names(results)) {
cat("\n", name, ":\n", sep = "")
print(results[[name]])
}
library(caret)    # For splitting the dataset and evaluating the model
library(car)      # For VIF and other regression diagnostics
library(dplyr)    # For data manipulation
library(GGally)   # For creating advanced scatter plot matrices and correlation plots
library(ggplot2)  # For visualizations
library(readr)    # For reading CSV files
library(tidyr)    # For reshaping and tidying data (e.g., pivoting data frames)
# Load the dataset
dataset <- read_csv("Housing.csv")
setwd("~/Documents/GitHub/BUAN348-448/Weeks 4-5")
# Load the dataset
dataset <- read_csv("Housing.csv")
head(dataset)
str(dataset)
summary(dataset)
# Data Cleaning
# Replace missing values (if any) with NA
dataset[dataset == ""] <- NA
# Impute missing numeric variables with the median
numeric_cols <- sapply(dataset, is.numeric)
dataset[numeric_cols] <- lapply(dataset[numeric_cols], function(x) {
ifelse(is.na(x), median(x, na.rm = TRUE), x)
})
# Check for duplicate rows and remove them
dataset <- dataset %>% distinct()
print(dataset)
# Convert categorical variables to factors
dataset <- dataset %>%
mutate(
mainroad = factor(mainroad, levels = c("no", "yes")),
guestroom = factor(guestroom, levels = c("no", "yes")),
basement = factor(basement, levels = c("no", "yes")),
hotwaterheating = factor(hotwaterheating, levels = c("no", "yes")),
airconditioning = factor(airconditioning, levels = c("no", "yes")),
prefarea = factor(prefarea, levels = c("no", "yes")),
furnishingstatus = factor(furnishingstatus, levels = c("unfurnished", "semi-furnished", "furnished"))
)
numeric_columns <- dataset %>% select(area, bedrooms, bathrooms, stories, parking, price)
numeric_columns_long <- numeric_columns %>%
pivot_longer(cols = -price, names_to = "variable", values_to = "value")
print(numeric_columns_long)
ggplot(numeric_columns_long, aes(x = variable, y = value)) +
geom_boxplot() +
ggtitle('Box Plot of Numeric Variables') +
xlab('Variables') +
ylab('Values') +
theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
facet_wrap(~variable, scales = "free_y")
# Scatter plot matrix
ggpairs(dataset %>% select(area, bedrooms, bathrooms, stories, parking, price),
title = "Scatter Plot Matrix of Selected Variables")
# Correlation heatmap
correlation_matrix <- cor(dataset %>% select_if(is.numeric))
melted_corr_matrix <- as.data.frame(as.table(correlation_matrix))
ggplot(data = melted_corr_matrix, aes(x = Var1, y = Var2, fill = Freq)) +
geom_tile() +
geom_text(aes(label = round(Freq, 2)), color = "black", size = 4) +  # Add correlation values
scale_fill_gradient2(low = "blue", high = "red", mid = "white",
midpoint = 0, limit = c(-1, 1), space = "Lab",
name = "Correlation") +
theme_minimal() +
ggtitle("Correlation Heatmap of Numeric Variables") +
xlab("Variables") +
ylab("Variables") +
theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1))
# Split the dataset into training and testing sets
set.seed(123)  # For reproducibility
trainIndex <- createDataPartition(dataset$price, p = 0.8, list = FALSE)
train_data <- dataset[trainIndex, ]
test_data <- dataset[-trainIndex, ]
# Fit the Multiple Linear Regression Model
model <- lm(price ~ area + bedrooms + bathrooms + stories + mainroad + guestroom + basement +
hotwaterheating + airconditioning + parking + prefarea + furnishingstatus, data = train_data)
# Model Summary
summary(model)
# Make predictions on the test data
predictions <- predict(model, newdata = test_data)
# Evaluate Model Performance
# Mean Squared Error (MSE) and Root Mean Squared Error (RMSE)
actual_values <- test_data$price
mse <- mean((actual_values - predictions)^2)
rmse <- sqrt(mse)
cat("Mean Squared Error (MSE): ", mse, "\n")
cat("Root Mean Squared Error (RMSE): ", rmse, "\n")
# R-squared and Adjusted R-squared
r_squared <- summary(model)$r.squared
adjusted_r_squared <- summary(model)$adj.r.squared
cat("R-squared: ", r_squared, "\n")
cat("Adjusted R-squared: ", adjusted_r_squared, "\n")
