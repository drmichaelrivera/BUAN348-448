ggtitle('Box Plot of Numeric Variables') +
xlab('Variables') +
ylab('Values') +
theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
facet_wrap(~variable, scales = "free_y")
correlation_matrix <- cor(dataset %>% select_if(is.numeric))
# Convert the correlation matrix to a long format
melted_corr_matrix <- as.data.frame(as.table(correlation_matrix))
# Plot the heatmap
ggplot(data = melted_corr_matrix, aes(x = Var1, y = Var2, fill = Freq)) +
geom_tile() +
scale_fill_gradient2(low = "blue", high = "red", mid = "white",
midpoint = 0, limit = c(-1,1), space = "Lab",
name="Correlation") +
theme_minimal() +
ggtitle("Correlation Heatmap of Numeric Variables") +
xlab("Variables") +
ylab("Variables") +
theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1))
# Step 4: Convert categorical variables to factors
dataset <- dataset %>%
mutate(
mainroad = factor(mainroad, levels = c("no", "yes")),
guestroom = factor(guestroom, levels = c("no", "yes")),
basement = factor(basement, levels = c("no", "yes")),
hotwaterheating = factor(hotwaterheating, levels = c("no", "yes")),
airconditioning = factor(airconditioning, levels = c("no", "yes")),
prefarea = factor(prefarea, levels = c("no", "yes")),
furnishingstatus = factor(furnishingstatus, levels = c("unfurnished", "semi-furnished", "furnished"))
)
correlation_matrix <- cor(dataset %>% select_if(is.numeric))
# Convert the correlation matrix to long format
melted_corr_matrix <- as.data.frame(as.table(correlation_matrix))
# Plot the heatmap with correlation values
ggplot(data = melted_corr_matrix, aes(x = Var1, y = Var2, fill = Freq)) +
geom_tile() +
geom_text(aes(label = round(Freq, 2)), color = "black", size = 4) +  # Add correlation values
scale_fill_gradient2(low = "blue", high = "red", mid = "white",
midpoint = 0, limit = c(-1, 1), space = "Lab",
name = "Correlation") +
theme_minimal() +
ggtitle("Correlation Heatmap of Numeric Variables") +
xlab("Variables") +
ylab("Variables") +
theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1))
pairs(~area + bedrooms + bathrooms + stories + parking + price, data = dataset,
main = "Scatter Plot Matrix")
########SCATTER PLOT MATRIX##############
ggpairs(dataset %>% select(area, bedrooms, bathrooms, stories, parking, price),
title = "Scatter Plot Matrix of Selected Variables")
head(dataset)
str(dataset)
summary(dataset)
# Step 4: Convert categorical variables to factors
dataset <- dataset %>%
mutate(
mainroad = factor(mainroad, levels = c("no", "yes")),
guestroom = factor(guestroom, levels = c("no", "yes")),
basement = factor(basement, levels = c("no", "yes")),
hotwaterheating = factor(hotwaterheating, levels = c("no", "yes")),
airconditioning = factor(airconditioning, levels = c("no", "yes")),
prefarea = factor(prefarea, levels = c("no", "yes")),
furnishingstatus = factor(furnishingstatus, levels = c("unfurnished", "semi-furnished", "furnished"))
)
# Step 5: Split the dataset into training and testing sets
set.seed(123)  # For reproducibility
trainIndex <- createDataPartition(dataset$price, p = 0.8, list = FALSE)
train_data <- dataset[trainIndex, ]
test_data <- dataset[-trainIndex, ]
# Step 6: Fit the Multiple Linear Regression Model
model <- lm(price ~ area + bedrooms + bathrooms + stories + mainroad + guestroom + basement +
hotwaterheating + airconditioning + parking + prefarea + furnishingstatus, data = train_data)
# Step 7: Model Summary
summary(model)
# Step 8: Make predictions on the test data
predictions <- predict(model, newdata = test_data)
# Step 9: Evaluate Model Performance
# 9.1: Mean Squared Error (MSE) and Root Mean Squared Error (RMSE)
actual_values <- test_data$price
mse <- mean((predictions - actual_values)^2)
rmse <- sqrt(mse)
cat("Mean Squared Error (MSE): ", mse, "\n")
cat("Root Mean Squared Error (RMSE): ", rmse, "\n")
# 9.2: R-squared and Adjusted R-squared
r_squared <- summary(model)$r.squared
adjusted_r_squared <- summary(model)$adj.r.squared
cat("R-squared: ", r_squared, "\n")
cat("Adjusted R-squared: ", adjusted_r_squared, "\n")
# Step 7: Model Summary
summary(model)
# Step 10: Data Visualization for MLR Assumptions
# 10.1: Linearity (scatter plot of actual vs predicted values)
ggplot(test_data, aes(x = predictions, y = actual_values)) +
geom_point(color = 'blue') +
geom_abline(slope = 1, intercept = 0, color = 'red') +
ggtitle('Actual vs Predicted Prices') +
xlab('Predicted Prices') +
ylab('Actual Prices')
ggplot(residuals_data, aes(x = fitted_values, y = residuals)) +
geom_point() +
geom_smooth(method = 'loess', color = 'red') +
ggtitle('Residuals vs Fitted Values') +
xlab('Fitted Values') +
ylab('Residuals')
# 10.3: Normality of Residuals (Q-Q plot)
ggplot(residuals_data, aes(sample = residuals)) +
stat_qq() +
stat_qq_line() +
ggtitle('Q-Q Plot of Residuals')
# 10.4: Multicollinearity (VIF)
vif_values <- vif(model)
print(vif_values)
library(glmnet)
# Prepare data for glmnet (it requires a matrix of predictors)
x_train <- model.matrix(price ~ area + bedrooms + bathrooms + stories + mainroad + guestroom + basement +
hotwaterheating + airconditioning + parking + prefarea + furnishingstatus, data = train_data)[, -1]
y_train <- train_data$price
x_test <- model.matrix(price ~ area + bedrooms + bathrooms + stories + mainroad + guestroom + basement +
hotwaterheating + airconditioning + parking + prefarea + furnishingstatus, data = test_data)[, -1]
y_test <- test_data$price
# 11.1: Ridge Regression
ridge_model <- cv.glmnet(x_train, y_train, alpha = 0)  # alpha = 0 for Ridge
ridge_predictions <- predict(ridge_model, s = ridge_model$lambda.min, newx = x_test)
ridge_mse <- mean((ridge_predictions - y_test)^2)
ridge_rmse <- sqrt(ridge_mse)
# 11.2: Lasso Regression
lasso_model <- cv.glmnet(x_train, y_train, alpha = 1)  # alpha = 1 for Lasso
lasso_predictions <- predict(lasso_model, s = lasso_model$lambda.min, newx = x_test)
lasso_mse <- mean((lasso_predictions - y_test)^2)
lasso_rmse <- sqrt(lasso_mse)
# 11.3: Elastic Net Regression
elasticnet_model <- cv.glmnet(x_train, y_train, alpha = 0.5)  # alpha = 0.5 for Elastic Net
elasticnet_predictions <- predict(elasticnet_model, s = elasticnet_model$lambda.min, newx = x_test)
elasticnet_mse <- mean((elasticnet_predictions - y_test)^2)
elasticnet_rmse <- sqrt(elasticnet_mse)
# Output key metrics for each model
cat("Ridge MSE: ", ridge_mse, "\nRidge RMSE: ", ridge_rmse, "\n")
cat("Lasso MSE: ", lasso_mse, "\nLasso RMSE: ", lasso_rmse, "\n")
cat("Elastic Net MSE: ", elasticnet_mse, "\nElastic Net RMSE: ", elasticnet_rmse, "\n")
# Step 12: Summary of all models' performance for comparison
# Collect all model metrics
model_comparison <- data.frame(
Model = c("Linear Regression", "Ridge Regression", "Lasso Regression", "Elastic Net Regression"),
MSE = c(mse, ridge_mse, lasso_mse, elasticnet_mse),
RMSE = c(rmse, ridge_rmse, lasso_rmse, elasticnet_rmse),
R_squared = c(r_squared, NA, NA, NA),  # R-squared only applies to linear regression
Adjusted_R_squared = c(adjusted_r_squared, NA, NA, NA)  # Adjusted R-squared only applies to linear regression
)
# Display the comparison of models
print(model_comparison)
# Step 13: Fine tuning
# Load the required package
library(glmnet)
# Prepare the data for glmnet
x_train <- model.matrix(price ~ area + bedrooms + bathrooms + stories + mainroad + guestroom + basement +
hotwaterheating + airconditioning + parking + prefarea + furnishingstatus, data = train_data)[, -1]
y_train <- train_data$price
x_test <- model.matrix(price ~ area + bedrooms + bathrooms + stories + mainroad + guestroom + basement +
hotwaterheating + airconditioning + parking + prefarea + furnishingstatus, data = test_data)[, -1]
y_test <- test_data$price
# Create a grid of lambda values (can modify as needed)
lambda_grid <- 10^seq(10, -2, length = 100)
# 1. Tuning Ridge Regression (alpha = 0)
ridge_model <- cv.glmnet(x_train, y_train, alpha = 0, lambda = lambda_grid)
best_lambda_ridge <- ridge_model$lambda.min  # Best lambda based on CV
cat("Best lambda for Ridge:", best_lambda_ridge, "\n")
ridge_predictions <- predict(ridge_model, s = best_lambda_ridge, newx = x_test)
ridge_mse <- mean((ridge_predictions - y_test)^2)
ridge_rmse <- sqrt(ridge_mse)
# 2. Tuning Lasso Regression (alpha = 1)
lasso_model <- cv.glmnet(x_train, y_train, alpha = 1, lambda = lambda_grid)
best_lambda_lasso <- lasso_model$lambda.min  # Best lambda based on CV
cat("Best lambda for Lasso:", best_lambda_lasso, "\n")
lasso_predictions <- predict(lasso_model, s = best_lambda_lasso, newx = x_test)
lasso_mse <- mean((lasso_predictions - y_test)^2)
lasso_rmse <- sqrt(lasso_mse)
# 3. Tuning Elastic Net (alpha between 0 and 1)
# We will try different values of alpha to find the best combination
alpha_values <- seq(0.1, 0.9, by = 0.1)
elasticnet_results <- data.frame(alpha = numeric(), best_lambda = numeric(), MSE = numeric())
for (alpha in alpha_values) {
elasticnet_model <- cv.glmnet(x_train, y_train, alpha = alpha, lambda = lambda_grid)
best_lambda_elasticnet <- elasticnet_model$lambda.min
elasticnet_predictions <- predict(elasticnet_model, s = best_lambda_elasticnet, newx = x_test)
elasticnet_mse <- mean((elasticnet_predictions - y_test)^2)
elasticnet_results <- rbind(elasticnet_results, data.frame(alpha = alpha, best_lambda = best_lambda_elasticnet, MSE = elasticnet_mse))
}
# Find the best alpha and lambda based on the lowest MSE
best_elasticnet <- elasticnet_results[which.min(elasticnet_results$MSE), ]
cat("Best alpha for Elastic Net:", best_elasticnet$alpha, "\n")
cat("Best lambda for Elastic Net:", best_elasticnet$best_lambda, "\n")
# Output performance metrics
cat("Ridge MSE: ", ridge_mse, "\nRidge RMSE: ", ridge_rmse, "\n")
cat("Lasso MSE: ", lasso_mse, "\nLasso RMSE: ", lasso_rmse, "\n")
cat("Elastic Net MSE: ", best_elasticnet$MSE, "\n")
########################################################################
# Problem (Not necessary to include this in your code file for the final
# project. However, this is a necessary phase and should be included in your
# presentation deck. I include it here to reinforce its connectedness and
# importance in the process.)
########################################################################
# Problem Statement:
# Zillow, a major player in real estate, has recently come under scrutiny due
# to significant fluctuations in housing prices in various regions, particularly
# in response to external factors like rising interest rates and inflation. These
# fluctuations have raised concerns about the accuracy of Zillow’s price prediction models,
# particularly their ability to provide reliable estimates for homes listed on the
# platform. Zillow’s leadership team has tasked the analytics team with analyzing
# historical housing data to understand the root causes of pricing inaccuracies and
# propose improvements to the pricing prediction model.
# Key Questions:
# How accurate are Zillow's current price predictions compared to actual selling prices?
#  --Code/Analysis Connection: Use the model's RMSE and R-squared to evaluate prediction performance.
# Which factors (e.g., area, number of bedrooms, bathrooms, etc.) have the most significant impact on price prediction errors?
#  --Code/Analysis Connection: Analyze the coefficients of the multiple regression model to determine which variables contribute most to price prediction.
# Are there any patterns of over- or under-prediction in specific areas or home types?
#  --Code/Analysis Connection: Visualize the residuals (prediction errors) to detect any bias or patterns.
# Do any model assumptions (e.g., linearity, multicollinearity) need to be addressed to improve prediction accuracy?
#  --Code/Analysis Connection: Use residual plots, VIF, and Q-Q plots to check for assumptions and model diagnostics.
# How can Zillow’s pricing algorithm be improved to adapt to current market volatility?
#  -- Code/Analysis Connection: Based on the insights gained from the analysis, explore alternative models (e.g., regularization techniques) to improve the model's robustness.
# Root Cause Analysis:
# 1. Why are Zillow’s pricing predictions sometimes inaccurate?
#    Because the model does not account for all market conditions (e.g., sudden interest rate hikes, regional demand fluctuations).
### 2. Why doesn't the model account for these market conditions?
###    The current variables in the model (e.g., area, number of bedrooms, etc.) are not sufficient to capture dynamic market factors like economic shifts or buyer sentiment.
##### 3. Why weren’t more dynamic factors included in the model?
#####    Zillow’s initial model was built based on static historical data, without incorporating real-time data or external economic indicators.
####### 4. Why is the model dependent on static historical data?
#######    The original algorithm was designed with a focus on past trends rather than adaptive mechanisms that can adjust based on changing market conditions.
######### 5. Why hasn’t the model been updated to include adaptive features?
#########    The decision to update the model has been delayed due to the complexity of incorporating real-time data and the cost of implementing new algorithms.
########################################################################
# DATA
########################################################################
# Load necessary libraries
library(caret)    # For splitting the dataset and evaluating the model
library(car)      # For VIF and other regression diagnostics
library(dplyr)    # For data manipulation
library(GGally)   # For creating advanced scatter plot matrices and correlation plots
library(ggplot2)  # For visualizations
library(readr)    # For reading CSV files
library(tidyr)    # For reshaping and tidying data (e.g., pivoting data frames)
# Step 1: Load the dataset
dataset <- read_csv("Housing.csv")
head(dataset)
str(dataset)
summary(dataset)
# Step 2: Data Cleaning
# 2.1: Replace missing values (if any) with NA
dataset[dataset == ""] <- NA
# 2.2: Impute missing numeric variables with the median
numeric_cols <- sapply(dataset, is.numeric)
dataset[numeric_cols] <- lapply(dataset[numeric_cols], function(x) {
ifelse(is.na(x), median(x, na.rm = TRUE), x)
})
# 2.3: Check for duplicate rows and remove them
dataset <- dataset %>% distinct()
print(dataset)
# Step 3: Data Visualization (Box plot, Scatter plot matrix, Heatmap)
# 3.1: Box plot for numeric variables
numeric_columns <- dataset %>% select(area, bedrooms, bathrooms, stories, parking, price)
numeric_columns_long <- numeric_columns %>%
pivot_longer(cols = -price, names_to = "variable", values_to = "value")
print(numeric_columns_long)
ggplot(numeric_columns_long, aes(x = variable, y = value)) +
geom_boxplot() +
ggtitle('Box Plot of Numeric Variables') +
xlab('Variables') +
ylab('Values') +
theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
facet_wrap(~variable, scales = "free_y")
# 3.2: Scatter plot matrix
ggpairs(dataset %>% select(area, bedrooms, bathrooms, stories, parking, price),
title = "Scatter Plot Matrix of Selected Variables")
# 3.3: Correlation heatmap
correlation_matrix <- cor(dataset %>% select_if(is.numeric))
melted_corr_matrix <- as.data.frame(as.table(correlation_matrix))
ggplot(data = melted_corr_matrix, aes(x = Var1, y = Var2, fill = Freq)) +
geom_tile() +
geom_text(aes(label = round(Freq, 2)), color = "black", size = 4) +  # Add correlation values
scale_fill_gradient2(low = "blue", high = "red", mid = "white",
midpoint = 0, limit = c(-1, 1), space = "Lab",
name = "Correlation") +
theme_minimal() +
ggtitle("Correlation Heatmap of Numeric Variables") +
xlab("Variables") +
ylab("Variables") +
theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1))
# Step 4: Convert categorical variables to factors
dataset <- dataset %>%
mutate(
mainroad = factor(mainroad, levels = c("no", "yes")),
guestroom = factor(guestroom, levels = c("no", "yes")),
basement = factor(basement, levels = c("no", "yes")),
hotwaterheating = factor(hotwaterheating, levels = c("no", "yes")),
airconditioning = factor(airconditioning, levels = c("no", "yes")),
prefarea = factor(prefarea, levels = c("no", "yes")),
furnishingstatus = factor(furnishingstatus, levels = c("unfurnished", "semi-furnished", "furnished"))
)
########################################################################
# MODEL
########################################################################
# Step 5: Split the dataset into training and testing sets
set.seed(123)  # For reproducibility
trainIndex <- createDataPartition(dataset$price, p = 0.8, list = FALSE)
train_data <- dataset[trainIndex, ]
test_data <- dataset[-trainIndex, ]
# Step 6: Fit the Multiple Linear Regression Model
model <- lm(price ~ area + bedrooms + bathrooms + stories + mainroad + guestroom + basement +
hotwaterheating + airconditioning + parking + prefarea + furnishingstatus, data = train_data)
# Step 7: Model Summary
summary(model)
# Step 8: Make predictions on the test data
predictions <- predict(model, newdata = test_data)
# Step 9: Evaluate Model Performance
# 9.1: Mean Squared Error (MSE) and Root Mean Squared Error (RMSE)
actual_values <- test_data$price
mse <- mean((predictions - actual_values)^2)
rmse <- sqrt(mse)
cat("Mean Squared Error (MSE): ", mse, "\n")
cat("Root Mean Squared Error (RMSE): ", rmse, "\n")
# 9.2: R-squared and Adjusted R-squared
r_squared <- summary(model)$r.squared
adjusted_r_squared <- summary(model)$adj.r.squared
cat("R-squared: ", r_squared, "\n")
cat("Adjusted R-squared: ", adjusted_r_squared, "\n")
# Step 10: Data Visualization for MLR Assumptions
# 10.1: Linearity (scatter plot of actual vs predicted values)
ggplot(test_data, aes(x = predictions, y = actual_values)) +
geom_point(color = 'blue') +
geom_abline(slope = 1, intercept = 0, color = 'red') +
ggtitle('Actual vs Predicted Prices') +
xlab('Predicted Prices') +
ylab('Actual Prices')
# 10.2: Homoscedasticity (residual plot)
residuals_data <- data.frame(fitted_values = fitted(model), residuals = resid(model))
ggplot(residuals_data, aes(x = fitted_values, y = residuals)) +
geom_point() +
geom_smooth(method = 'loess', color = 'red') +
ggtitle('Residuals vs Fitted Values') +
xlab('Fitted Values') +
ylab('Residuals')
# 10.3: Normality of Residuals (Q-Q plot)
ggplot(residuals_data, aes(sample = residuals)) +
stat_qq() +
stat_qq_line() +
ggtitle('Q-Q Plot of Residuals')
# 10.4: Multicollinearity (VIF)
vif_values <- vif(model)
print(vif_values)
########################################################################
# REGULARIZATION
########################################################################
# We make this a separate section here to support clarity, but we very well could have this connected to our model building process.
# Step 11: Regularization using Ridge, Lasso, and Elastic Net
library(glmnet)
# Prepare data for glmnet (it requires a matrix of predictors)
x_train <- model.matrix(price ~ area + bedrooms + bathrooms + stories + mainroad + guestroom + basement +
hotwaterheating + airconditioning + parking + prefarea + furnishingstatus, data = train_data)[, -1]
y_train <- train_data$price
x_test <- model.matrix(price ~ area + bedrooms + bathrooms + stories + mainroad + guestroom + basement +
hotwaterheating + airconditioning + parking + prefarea + furnishingstatus, data = test_data)[, -1]
y_test <- test_data$price
# 11.1: Ridge Regression
ridge_model <- cv.glmnet(x_train, y_train, alpha = 0)  # alpha = 0 for Ridge
ridge_predictions <- predict(ridge_model, s = ridge_model$lambda.min, newx = x_test)
ridge_mse <- mean((ridge_predictions - y_test)^2)
ridge_rmse <- sqrt(ridge_mse)
# 11.2: Lasso Regression
lasso_model <- cv.glmnet(x_train, y_train, alpha = 1)  # alpha = 1 for Lasso
lasso_predictions <- predict(lasso_model, s = lasso_model$lambda.min, newx = x_test)
lasso_mse <- mean((lasso_predictions - y_test)^2)
lasso_rmse <- sqrt(lasso_mse)
# 11.3: Elastic Net Regression
elasticnet_model <- cv.glmnet(x_train, y_train, alpha = 0.5)  # alpha = 0.5 for Elastic Net
elasticnet_predictions <- predict(elasticnet_model, s = elasticnet_model$lambda.min, newx = x_test)
elasticnet_mse <- mean((elasticnet_predictions - y_test)^2)
elasticnet_rmse <- sqrt(elasticnet_mse)
# Output key metrics for each model
cat("Ridge MSE: ", ridge_mse, "\nRidge RMSE: ", ridge_rmse, "\n")
cat("Lasso MSE: ", lasso_mse, "\nLasso RMSE: ", lasso_rmse, "\n")
cat("Elastic Net MSE: ", elasticnet_mse, "\nElastic Net RMSE: ", elasticnet_rmse, "\n")
# Step 12: Summary of all models' performance for comparison
# Collect all model metrics
model_comparison <- data.frame(
Model = c("Linear Regression", "Ridge Regression", "Lasso Regression", "Elastic Net Regression"),
MSE = c(mse, ridge_mse, lasso_mse, elasticnet_mse),
RMSE = c(rmse, ridge_rmse, lasso_rmse, elasticnet_rmse),
R_squared = c(r_squared, NA, NA, NA),  # R-squared only applies to linear regression
Adjusted_R_squared = c(adjusted_r_squared, NA, NA, NA)  # Adjusted R-squared only applies to linear regression
)
# Display the comparison of models
print(model_comparison)
########################################################################
# FINE TUNING/EXPERIMENTATION
########################################################################
# Here, we can fine tune the models by adjusting included features and alpha levels.
# and experimenting. We make this a separate section here to support clarity,
# but we very well could have this connected to our model building process.
# Step 13: Fine tuning and experimentation
# Load the required package
library(glmnet)
# Prepare the data for glmnet
x_train <- model.matrix(price ~ area + bedrooms + bathrooms + stories + mainroad + guestroom + basement +
hotwaterheating + airconditioning + parking + prefarea + furnishingstatus, data = train_data)[, -1]
y_train <- train_data$price
x_test <- model.matrix(price ~ area + bedrooms + bathrooms + stories + mainroad + guestroom + basement +
hotwaterheating + airconditioning + parking + prefarea + furnishingstatus, data = test_data)[, -1]
y_test <- test_data$price
# Create a grid of lambda values (can modify as needed)
lambda_grid <- 10^seq(10, -2, length = 100)
# 13.1 Tuning Ridge Regression (alpha = 0)
ridge_model <- cv.glmnet(x_train, y_train, alpha = 0, lambda = lambda_grid)
best_lambda_ridge <- ridge_model$lambda.min  # Best lambda based on CV
cat("Best lambda for Ridge:", best_lambda_ridge, "\n")
ridge_predictions <- predict(ridge_model, s = best_lambda_ridge, newx = x_test)
ridge_mse <- mean((ridge_predictions - y_test)^2)
ridge_rmse <- sqrt(ridge_mse)
# 13.2 Tuning Lasso Regression (alpha = 1)
lasso_model <- cv.glmnet(x_train, y_train, alpha = 1, lambda = lambda_grid)
best_lambda_lasso <- lasso_model$lambda.min  # Best lambda based on CV
cat("Best lambda for Lasso:", best_lambda_lasso, "\n")
lasso_predictions <- predict(lasso_model, s = best_lambda_lasso, newx = x_test)
lasso_mse <- mean((lasso_predictions - y_test)^2)
lasso_rmse <- sqrt(lasso_mse)
# 13.3 Tuning Elastic Net (alpha between 0 and 1)
# We will try different values of alpha to find the best combination
alpha_values <- seq(0.1, 0.9, by = 0.1)
elasticnet_results <- data.frame(alpha = numeric(), best_lambda = numeric(), MSE = numeric())
for (alpha in alpha_values) {
elasticnet_model <- cv.glmnet(x_train, y_train, alpha = alpha, lambda = lambda_grid)
best_lambda_elasticnet <- elasticnet_model$lambda.min
elasticnet_predictions <- predict(elasticnet_model, s = best_lambda_elasticnet, newx = x_test)
elasticnet_mse <- mean((elasticnet_predictions - y_test)^2)
elasticnet_results <- rbind(elasticnet_results, data.frame(alpha = alpha, best_lambda = best_lambda_elasticnet, MSE = elasticnet_mse))
}
# Find the best alpha and lambda based on the lowest MSE
best_elasticnet <- elasticnet_results[which.min(elasticnet_results$MSE), ]
cat("Best alpha for Elastic Net:", best_elasticnet$alpha, "\n")
cat("Best lambda for Elastic Net:", best_elasticnet$best_lambda, "\n")
# Output performance metrics
cat("Ridge MSE: ", ridge_mse, "\nRidge RMSE: ", ridge_rmse, "\n")
cat("Lasso MSE: ", lasso_mse, "\nLasso RMSE: ", lasso_rmse, "\n")
cat("Elastic Net MSE: ", best_elasticnet$MSE, "\n")
########################################################################
# IMPACT (Not necessary to include this in your code file for the final
# project. However, this is a necessary phase and should be included in your
# presentation deck. I include it here to reinforce its connectedness and
# importance in the process.)
########################################################################
# Go back to the mini case and focus on answering the questions.
# Share your findings and suggest a managerial recommendation.
# Two sentences should suffice for each. We find...  Therefore, we suggest...
# Try to tie some aspect of the managerial recommendation to your 5 Whys drivers.
# When finished, we will debrief this mini case as a group.
# Step 7: Model Summary
summary(model)
# Step 8: Make predictions on the test data
predictions <- predict(model, newdata = test_data)
# Step 9: Evaluate Model Performance
# 9.1: Mean Squared Error (MSE) and Root Mean Squared Error (RMSE)
actual_values <- test_data$price
mse <- mean((predictions - actual_values)^2)
rmse <- sqrt(mse)
cat("Mean Squared Error (MSE): ", mse, "\n")
cat("Root Mean Squared Error (RMSE): ", rmse, "\n")
# Step 8: Make predictions on the test data
predictions <- predict(model, newdata = test_data)
# Step 9: Evaluate Model Performance
# 9.1: Mean Squared Error (MSE) and Root Mean Squared Error (RMSE)
actual_values <- test_data$price
mse <- mean((predictions - actual_values)^2)
rmse <- sqrt(mse)
cat("Mean Squared Error (MSE): ", mse, "\n")
cat("Root Mean Squared Error (RMSE): ", rmse, "\n")
# 9.2: R-squared and Adjusted R-squared
r_squared <- summary(model)$r.squared
adjusted_r_squared <- summary(model)$adj.r.squared
cat("R-squared: ", r_squared, "\n")
cat("Adjusted R-squared: ", adjusted_r_squared, "\n")
# Step 9: Evaluate Model Performance
# 9.1: Mean Squared Error (MSE) and Root Mean Squared Error (RMSE)
actual_values <- test_data$price
mse <- mean((predictions - actual_values)^2)
rmse <- sqrt(mse)
cat("Mean Squared Error (MSE): ", mse, "\n")
cat("Root Mean Squared Error (RMSE): ", rmse, "\n")
# 9.2: R-squared and Adjusted R-squared
r_squared <- summary(model)$r.squared
adjusted_r_squared <- summary(model)$adj.r.squared
cat("R-squared: ", r_squared, "\n")
cat("Adjusted R-squared: ", adjusted_r_squared, "\n")
# 10.4 Independence
# Perform the Durbin-Watson test
durbinWatsonTest(model)
# Step 13: Set up cross-validation
train_control <- trainControl(method = "cv", number = 10)  # 10-fold cross-validation
# 13.1 Fit the Multiple Linear Regression Model with cross-validation
model <- train(
price ~ area + bedrooms + bathrooms + stories + mainroad + guestroom + basement +
hotwaterheating + airconditioning + parking + prefarea + furnishingstatus,
data = dataset,
method = "lm",
trControl = train_control
)
# Step 13.2 View model summary
summary(model)
# Evaluate model performance using cross-validation results
cat("Cross-validated RMSE: ", model$results$RMSE, "\n")
cat("Cross-validated R-squared: ", model$results$Rsquared, "\n")
load("/Users/michaelrivera/Documents/GitHub/BUAN348-448/.RData")
